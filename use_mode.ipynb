{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import random\n",
    "import cv2\n",
    "import src.models as mdls\n",
    "import src.utils as utils\n",
    "from numpy import inf\n",
    "import torchvision\n",
    "import torchvision.transforms as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mdls.InspectorGadjet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load('/Users/andreisuhov/Desktop/checkpoint_epoch_5.pth', map_location=torch.device('cpu'))['model_state_dict']\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreisuhov/Desktop/DL projects/Face_id_and_detection/use_mode.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andreisuhov/Desktop/DL%20projects/Face_id_and_detection/use_mode.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m utils\u001b[39m.\u001b[39;49mcam_capture(\u001b[39m0\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel)\n",
      "File \u001b[0;32m~/Desktop/DL projects/Face_id_and_detection/src/utils.py:308\u001b[0m, in \u001b[0;36mcam_capture\u001b[0;34m(source, model, bbox_func, limit)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    306\u001b[0m     res \u001b[39m=\u001b[39m model(pic_tens\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[0;32m--> 308\u001b[0m coord \u001b[39m=\u001b[39m rescale_coordinates(res)\n\u001b[1;32m    311\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(frame, (coord[\u001b[39m0\u001b[39m], coord[\u001b[39m1\u001b[39m]), (coord[\u001b[39m2\u001b[39m], coord[\u001b[39m3\u001b[39m]), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m    313\u001b[0m \u001b[39mprint\u001b[39m(coord)\n",
      "File \u001b[0;32m~/Desktop/DL projects/Face_id_and_detection/src/utils.py:259\u001b[0m, in \u001b[0;36mrescale_coordinates\u001b[0;34m(tensor, original_shape, model_shape)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrescale_coordinates\u001b[39m(tensor, original_shape\u001b[39m=\u001b[39m(\u001b[39m720\u001b[39m, \u001b[39m1280\u001b[39m), model_shape\u001b[39m=\u001b[39m(\u001b[39m126\u001b[39m, \u001b[39m126\u001b[39m)):\n\u001b[1;32m    258\u001b[0m     \u001b[39m# Извлекаем координаты из тензора\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     x1, y1, x2, y2 \u001b[39m=\u001b[39m tensor[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m    261\u001b[0m     \u001b[39m# Масштабирование координат\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((x1 \u001b[39m/\u001b[39m model_shape[\u001b[39m1\u001b[39m]) \u001b[39m*\u001b[39m original_shape[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 0)"
     ]
    }
   ],
   "source": [
    "utils.cam_capture(0, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
